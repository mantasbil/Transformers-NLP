# Transformers for NLP

This notebook is based on course work for a university course on Machine Learning. It demonstrates two natural language processing tasks using transformer models:

1.  **Text Similarity:** We use a pretrained BERT model to compute the semantic similarity between pairs of sentences using cosine similarity.
2.  **Sentiment Classification:** We fine-tune a pretrained RoBERTa model to classify text as either positive or negative. The notebook includes steps for data loading, preprocessing, model training, validation, and making predictions on unseen test data.

3.  The notebook also includes visualizations of the text similarity matrix for a small set of sentences.

This notebook focuses more on the code itself as opposed to analyzing data and ensuring highest model accuracy.
